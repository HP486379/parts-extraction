from __future__ import annotations

import os
import re
import math
import json
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import numpy as np
import camelot

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from fastapi import FastAPI, UploadFile, File, Form, HTTPException


# ======================
# FastAPI
# ======================

app = FastAPI(title="Parts Extractor API (Tkinter-logic port)")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ======================
# Patterns (ã‚†ã‚‹ãæ‹¾ã†)
# ======================

# éƒ¨å“ç•ªå·ï¼šãƒã‚¤ãƒ•ãƒ³/ã‚¹ãƒ©ãƒƒã‚·ãƒ¥/è‹±æ•°å­—/æœ«å°¾æç•ªãªã©ã‚’è¨±å®¹ï¼ˆfullmatchã—ãªã„ï¼‰
# ä¾‹: "ABC-123", "12-3456-78", "A12B-3", "12345", "X-12/34"
PART_NO_LOOSE = re.compile(r"[A-Z0-9][A-Z0-9\-\/]*[A-Z0-9]", re.IGNORECASE)

# L/W/Tã£ã½ã„æ•°å€¤ï¼š2, 2.0, 2.00, 2mm, t=2 ãªã©
NUM_LOOSE = re.compile(r"[-+]?\d+(?:\.\d+)?")

# ãƒ˜ãƒƒãƒ€æ¨å®šï¼šä¾å­˜ã—ã™ããªã„ï¼ˆè£œåŠ©ã¨ã—ã¦ä½¿ã†ï¼‰
HDR_L = re.compile(r"^(?:L|LENGTH|é•·ã•)\b", re.IGNORECASE)
HDR_W = re.compile(r"^(?:W|WIDTH|å¹…)\b", re.IGNORECASE)
HDR_T = re.compile(r"^(?:T|THK|THICK|åš|åšã¿)\b", re.IGNORECASE)
HDR_PART = re.compile(r"(?:å“ç•ª|éƒ¨å“|PART\s*NO|PART\s*NUMBER)", re.IGNORECASE)


# ======================
# Request/Response Models
# ======================

class SearchRequest(BaseModel):
    pdf_path: str = Field(..., description="ã‚µãƒ¼ãƒä¸Šã®PDFãƒ‘ã‚¹ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹/ãƒã‚¦ãƒ³ãƒˆãƒ‘ã‚¹ï¼‰")
    # ãƒšãƒ¼ã‚¸æŒ‡å®šï¼ˆNoneãªã‚‰å…¨éƒ¨ï¼‰
    pages: Optional[str] = Field(None, description='ä¾‹: "1,2,3" or "1-3"')
    # å·¦å³åˆ†å‰²ï¼ˆæ¯”ç‡ï¼‰
    split_ratio: float = Field(0.5, ge=0.2, le=0.8, description="ãƒšãƒ¼ã‚¸å¹…ã«å¯¾ã™ã‚‹å·¦å³å¢ƒç•Œã®æ¯”ç‡")
    # flavor: stream/lattice/auto
    flavor: str = Field("auto", description='camelot flavor: "auto"|"stream"|"lattice"')
    # ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæ•°å€¤æ¯”è¼ƒï¼‰
    L: Optional[float] = None
    W: Optional[float] = None
    T: Optional[float] = None
    tol: float = Field(0.05, ge=0.0, le=1.0, description="æ•°å€¤æ¯”è¼ƒã®è¨±å®¹å·®ï¼ˆç›¸å¯¾ï¼‰")

class PartRow(BaseModel):
    part_no: str
    L: Optional[float] = None
    W: Optional[float] = None
    T: Optional[float] = None
    raw: Dict[str, Any] = Field(default_factory=dict)

class SearchResponse(BaseModel):
    items: List[PartRow]
    debug: Dict[str, Any]


# ======================
# Core: Normalization
# ======================

def normalize_text(s: Any) -> str:
    if s is None:
        return ""
    s = str(s)
    # ä¸å¯è¦–/æ”¹è¡Œ/ã‚¿ãƒ–â†’ã‚¹ãƒšãƒ¼ã‚¹
    s = s.replace("\u00a0", " ").replace("\t", " ").replace("\r", " ").replace("\n", " ")
    # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹
    s = s.replace("\u3000", " ")
    # é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹ã‚’æ½°ã™
    s = re.sub(r"\s+", " ", s).strip()
    return s

def extract_first_number(s: Any) -> Optional[float]:
    s = normalize_text(s)
    if not s:
        return None
    m = NUM_LOOSE.search(s)
    if not m:
        return None
    try:
        return float(m.group(0))
    except ValueError:
        return None

def is_close(a: Optional[float], b: Optional[float], rel_tol: float) -> bool:
    if a is None or b is None:
        return False
    # 0ä»˜è¿‘ã ã‘ã¯çµ¶å¯¾èª¤å·®ã£ã½ã
    if abs(b) < 1e-9:
        return abs(a - b) <= rel_tol
    return abs(a - b) <= abs(b) * rel_tol

def score_part_candidate(cell: str) -> float:
    """
    éƒ¨å“ç•ªå·ã‚»ãƒ«å€™è£œã®ã‚¹ã‚³ã‚¢
    - å½¢ãŒãã‚Œã£ã½ã„
    - é•·ã™ã/çŸ­ã™ãã‚’æŠ‘åˆ¶
    - æ•°å€¤ã ã‘ã‚ˆã‚Šè‹±æ•°æ··åœ¨ã‚’å°‘ã—å„ªé‡ï¼ˆç¾å®Ÿå¯„ã‚Šï¼‰
    """
    t = normalize_text(cell)
    if not t:
        return 0.0
    if not PART_NO_LOOSE.search(t):
        return 0.0

    length = len(t)
    score = 1.0
    # é•·ã•ãƒšãƒŠãƒ«ãƒ†ã‚£
    if length < 4:
        score *= 0.4
    elif length > 30:
        score *= 0.5

    # è‹±å­—å«ã‚€ãªã‚‰å°‘ã—åŠ ç‚¹
    if re.search(r"[A-Z]", t, re.IGNORECASE):
        score *= 1.2

    # æ•°å€¤ã ã‘ã ã¨å°‘ã—æ¸›ç‚¹ï¼ˆãŸã ã—ã‚¼ãƒ­ã«ã¯ã—ãªã„ï¼‰
    if re.fullmatch(r"\d+(?:\.\d+)?", t):
        score *= 0.8

    # ä½™è¨ˆãªè¨˜å·ãŒå¤šã™ããŸã‚‰æ¸›ç‚¹
    if re.search(r"[=,:;]", t):
        score *= 0.7

    return float(score)

def score_dim_candidate(cell: str) -> float:
    """
    L/W/Tã‚»ãƒ«å€™è£œã®ã‚¹ã‚³ã‚¢ï¼ˆæ•°å€¤ãŒå–ã‚Œã‚‹ã»ã©é«˜ã„ï¼‰
    """
    n = extract_first_number(cell)
    if n is None:
        return 0.0
    # ç¾å®Ÿçš„ãªå¯¸æ³•ãƒ¬ãƒ³ã‚¸ã‚’è»½ãå„ªé‡ï¼ˆé›‘ã§OKï¼‰
    if 0 < n < 100000:
        return 1.0
    return 0.6


# ======================
# Core: Camelot wrapper (flavor auto)
# ======================

def camelot_read(pdf_path: str, pages: str, flavor: str):
    # ğŸš€ é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚ãªãŸã®PDFå‰æï¼‰
    return list(
        camelot.read_pdf(
            pdf_path,
            pages="1",          # 1ãƒšãƒ¼ã‚¸å›ºå®š
            flavor="lattice",   # ç½«ç·šã‚ã‚Šå›ºå®š
            line_scale=40,
            strip_text="\n",
        )
    )


# ======================
# Core: Table -> page split (å·¦å³åˆ†å‰²ã¯åº§æ¨™/æ¯”ç‡ã§æ±ºã‚ã‚‹)
# ======================

@dataclass
class TableBlock:
    side: str  # "L" or "R"
    df: pd.DataFrame
    meta: Dict[str, Any]

def split_table_left_right(df: pd.DataFrame, split_ratio: float, debug: Dict[str, Any], table_idx: int) -> List[TableBlock]:
    """
    Camelotã®dfã¯ã€Œè¦‹ãŸç›®ã®åˆ—ã€ãŒæ—¢ã«å…¥ã£ã¦ãã‚‹ãŒã€ãã‚ŒãŒã‚ºãƒ¬ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚
    Tkinterå¯„ã›ã¨ã—ã¦ã€Œå·¦å³ã€ã‚’åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ¯”ç‡ã§åˆ†ã‘ã‚‹ï¼ˆæœ€å°é™ã®ä¸å¤‰æ¡ä»¶ï¼‰ã€‚
    â€»æœ¬æ°—ã§åº§æ¨™ã‚’ä½¿ã†ãªã‚‰ã€Camelotã®_tableã‚„parsing_reportã«ä¾å­˜ã—ã‚„ã™ã„ã®ã§ã“ã“ã§ã¯å …ç‰¢ã•å„ªå…ˆã§ç°¡æ˜“ã€‚
    """
    df2 = df.copy()
    df2 = df2.applymap(normalize_text)

    ncols = df2.shape[1]
    cut = max(1, min(ncols - 1, int(math.floor(ncols * split_ratio))))

    left = df2.iloc[:, :cut]
    right = df2.iloc[:, cut:]

    debug["tables"][table_idx]["split"] = {
        "ncols": ncols,
        "cut_col_index": cut,
        "left_cols": left.shape[1],
        "right_cols": right.shape[1],
    }

    blocks = []
    if left.shape[1] > 0 and left.shape[0] > 0:
        blocks.append(TableBlock("L", left, {"table_idx": table_idx}))
    if right.shape[1] > 0 and right.shape[0] > 0:
        blocks.append(TableBlock("R", right, {"table_idx": table_idx}))
    return blocks


# ======================
# Core: Column inference (ãƒ˜ãƒƒãƒ€ã«ä¾å­˜ã—ã™ããªã„)
# ======================

@dataclass
class InferredColumns:
    part_col: int
    l_col: Optional[int] = None
    w_col: Optional[int] = None
    t_col: Optional[int] = None
    confidence: Dict[str, Any] = None

def infer_columns(df: pd.DataFrame) -> Optional[InferredColumns]:
    """
    1) éƒ¨å“ç•ªå·åˆ—ï¼šå…¨ã‚»ãƒ«ã‚’ã‚¹ã‚³ã‚¢ã—ã¦åˆ—åˆè¨ˆãŒæœ€å¤§ã®åˆ—ã‚’æ¡ç”¨
    2) L/W/Tåˆ—ï¼šãƒ˜ãƒƒãƒ€ãŒã‚ã‚Œã°å¼·ãã€ãªã‘ã‚Œã°æ•°å€¤å¯†åº¦ã§æ¨å®š
    """
    if df.empty:
        return None

    # ãƒ˜ãƒƒãƒ€è¡Œå€™è£œï¼šå…ˆé ­1ã€œ2è¡Œã‚’è¦‹ã¦ã€Œãƒ˜ãƒƒãƒ€ã£ã½ã•ã€ã‚’æ‹¾ã†ï¼ˆä¾å­˜ã¯å¼±ã‚ï¼‰
    header_rows = [0]
    if df.shape[0] >= 2:
        header_rows.append(1)

    col_scores_part = []
    col_scores_dim = []

    for c in range(df.shape[1]):
        col = df.iloc[:, c].astype(str).map(normalize_text)

        # PARTåˆ—ã‚¹ã‚³ã‚¢ï¼šå…¨è¡Œã‹ã‚‰
        s_part = col.map(score_part_candidate).sum()

        # DIMåˆ—ã‚¹ã‚³ã‚¢ï¼šå…¨è¡Œã‹ã‚‰ï¼ˆæ•°å€¤å–ã‚Œã‚‹å¯†åº¦ï¼‰
        s_dim = col.map(score_dim_candidate).sum()

        # ãƒ˜ãƒƒãƒ€åŠ ç‚¹
        hdr_text = " ".join([normalize_text(df.iat[r, c]) for r in header_rows if r < df.shape[0]])
        if HDR_PART.search(hdr_text):
            s_part *= 1.5
        if HDR_L.search(hdr_text) or HDR_W.search(hdr_text) or HDR_T.search(hdr_text):
            s_dim *= 1.2

        col_scores_part.append(float(s_part))
        col_scores_dim.append(float(s_dim))

    part_col = int(np.argmax(col_scores_part))
    # éƒ¨å“ç•ªå·åˆ—ãŒå¼±ã™ãã‚‹ãªã‚‰ Noneï¼ˆãŸã ã—é–¾å€¤ã¯ä½ã‚ï¼å€™è£œä¿æŒï¼‰
    if col_scores_part[part_col] < 1.0:
        return None

    # L/W/Tã¯ã€Œãƒ˜ãƒƒãƒ€å„ªå…ˆã€ç„¡ã‘ã‚Œã°dimã‚¹ã‚³ã‚¢ä¸Šä½ã‹ã‚‰å‰²å½“ã€
    l_col = w_col = t_col = None

    # ãƒ˜ãƒƒãƒ€ã§æ˜ç¢ºã«æŒ‡ã›ã‚‹ãªã‚‰æ¡ç”¨
    for c in range(df.shape[1]):
        hdr_text = " ".join([normalize_text(df.iat[r, c]) for r in header_rows if r < df.shape[0]])
        if l_col is None and HDR_L.search(hdr_text):
            l_col = c
        if w_col is None and HDR_W.search(hdr_text):
            w_col = c
        if t_col is None and HDR_T.search(hdr_text):
            t_col = c

    # æœªç¢ºå®šã¯ dimã‚¹ã‚³ã‚¢ã®é †ä½ã§åŸ‹ã‚ã‚‹ï¼ˆpart_colã¯é™¤å¤–ï¼‰
    order = np.argsort(col_scores_dim)[::-1].tolist()
    order = [c for c in order if c != part_col]

    def pick_next(exclude: set) -> Optional[int]:
        for c in order:
            if c not in exclude and col_scores_dim[c] >= 1.0:
                return int(c)
        return None

    used = {part_col}
    if l_col is None:
        l_col = pick_next(used)
        if l_col is not None:
            used.add(l_col)
    if w_col is None:
        w_col = pick_next(used)
        if w_col is not None:
            used.add(w_col)
    if t_col is None:
        t_col = pick_next(used)
        if t_col is not None:
            used.add(t_col)

    return InferredColumns(
        part_col=part_col,
        l_col=l_col,
        w_col=w_col,
        t_col=t_col,
        confidence={
            "part_scores": col_scores_part,
            "dim_scores": col_scores_dim,
            "chosen": {"part": part_col, "L": l_col, "W": w_col, "T": t_col},
        },
    )


# ======================
# Core: Row extraction (å³æ­»ã—ãªã„ã€å€™è£œä¿æŒâ†’æ­£è¦åŒ–)
# ======================

def extract_rows(df: pd.DataFrame, cols: InferredColumns) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    if df.empty:
        return rows

    for r in range(df.shape[0]):
        part_raw = normalize_text(df.iat[r, cols.part_col])

        # å€™è£œæ¡ä»¶ï¼šPARTã£ã½ã„â€œä½•ã‹â€ãŒå«ã¾ã‚Œã¦ã‚Œã°æ‹¾ã†ï¼ˆfullmatchç¦æ­¢ï¼‰
        if not PART_NO_LOOSE.search(part_raw):
            continue

        # æ­£è¦åŒ–ï¼šä½™è¨ˆãªã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤ã€é€£ç¶šè¨˜å·æ•´ç†ãªã©ï¼ˆå¿…è¦ãªã‚‰ã“ã“ã‚’åšãï¼‰
        part_no = part_raw.replace(" ", "")
        part_no = part_no.strip()

        L = extract_first_number(df.iat[r, cols.l_col]) if cols.l_col is not None else None
        W = extract_first_number(df.iat[r, cols.w_col]) if cols.w_col is not None else None
        T = extract_first_number(df.iat[r, cols.t_col]) if cols.t_col is not None else None

        rows.append(
            {
                "part_no": part_no,
                "L": L,
                "W": W,
                "T": T,
                "row_index": r,
                "raw": {
                    "part_cell": normalize_text(df.iat[r, cols.part_col]),
                    "L_cell": normalize_text(df.iat[r, cols.l_col]) if cols.l_col is not None else "",
                    "W_cell": normalize_text(df.iat[r, cols.w_col]) if cols.w_col is not None else "",
                    "T_cell": normalize_text(df.iat[r, cols.t_col]) if cols.t_col is not None else "",
                },
            }
        )

    return rows


def apply_numeric_filters(items: List[Dict[str, Any]], L: Optional[float], W: Optional[float], T: Optional[float], tol: float) -> List[Dict[str, Any]]:
    if L is None and W is None and T is None:
        return items

    out = []
    for it in items:
        ok = True
        if L is not None:
            ok = ok and is_close(it.get("L"), L, tol)
        if W is not None:
            ok = ok and is_close(it.get("W"), W, tol)
        if T is not None:
            ok = ok and is_close(it.get("T"), T, tol)
        if ok:
            out.append(it)
    return out


# ======================
# Public API: extract
# ======================

def normalize_pages(pages: Optional[str]) -> str:
    return "1"


def extract_parts_from_pdf(
    pdf_path: str,
    pages: Optional[str],
    split_ratio: float,
    flavor: str,
    flt_L: Optional[float],
    flt_W: Optional[float],
    flt_T: Optional[float],
    tol: float,
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:

    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"PDF not found: {pdf_path}")

    pages_s = normalize_pages(pages)

    debug: Dict[str, Any] = {
        "pdf_path": pdf_path,
        "pages": pages_s,
        "flavor": flavor,
        "split_ratio": split_ratio,
        "filters": {"L": flt_L, "W": flt_W, "T": flt_T, "tol": tol},
        "tables": [],
        "notes": [],
    }

    # Camelot read
    try:
        tables = camelot_read(pdf_path, pages_s, flavor)
    except Exception as e:
        debug["notes"].append(f"camelot_read failed: {repr(e)}")
        return [], debug

    debug["tables_count"] = len(tables)

    all_items: List[Dict[str, Any]] = []

    for i, t in enumerate(tables):
        try:
            df = t.df
        except Exception as e:
            debug["tables"].append({"table_idx": i, "error": repr(e)})
            continue

        tbl_dbg = {
            "table_idx": i,
            "shape": {"rows": int(df.shape[0]), "cols": int(df.shape[1])},
            "parsing_report": getattr(t, "parsing_report", None),
        }
        debug["tables"].append(tbl_dbg)

        # å·¦å³åˆ†å‰²ï¼ˆä¸å¤‰æ¡ä»¶ï¼‰
        blocks = split_table_left_right(df, split_ratio, debug, i)

        for b in blocks:
            cols = infer_columns(b.df)
            if cols is None:
                tbl_dbg.setdefault("blocks", []).append({"side": b.side, "status": "no_part_col"})
                continue

            tbl_dbg.setdefault("blocks", []).append(
                {
                    "side": b.side,
                    "status": "ok",
                    "inferred": cols.confidence,
                }
            )

            items = extract_rows(b.df, cols)

            # ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæ•°å€¤æ¯”è¼ƒï¼‰
            items2 = apply_numeric_filters(items, flt_L, flt_W, flt_T, tol)

            # 0ä»¶ã§ã‚‚åŸå› ãŒè¦‹ãˆã‚‹ã‚ˆã†ã«ã‚«ã‚¦ãƒ³ãƒˆ
            tbl_dbg["blocks"][-1]["extracted_rows"] = len(items)
            tbl_dbg["blocks"][-1]["after_filter_rows"] = len(items2)

            # side/table_idx ã‚’ä»˜åŠ ã—ã¦è¿½è·¡å¯èƒ½ã«
            for it in items2:
                it["source"] = {"table_idx": i, "side": b.side}
                all_items.append(it)

    return all_items, debug


# ======================
# Endpoint
# ======================

@app.post("/search", response_model=SearchResponse)
def search(req: SearchRequest):
    try:
        items, debug = extract_parts_from_pdf(
            pdf_path=req.pdf_path,
            pages=req.pages,
            split_ratio=req.split_ratio,
            flavor=req.flavor,
            flt_L=req.L,
            flt_W=req.W,
            flt_T=req.T,
            tol=req.tol,
        )
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal error: {repr(e)}")

    # è¿”å´æ•´å½¢
    out = [
        {
            "part_no": it["part_no"],
            "L": it.get("L"),
            "W": it.get("W"),
            "T": it.get("T"),
            "raw": {"source": it.get("source"), **it.get("raw", {})},
        }
        for it in items
    ]

    return {"items": out, "debug": debug}

import tempfile
from concurrent.futures import ProcessPoolExecutor, as_completed

@app.post("/api/extract_part_numbers_from_table")
async def extract_part_numbers_from_table(
    files: List[UploadFile] = File(...),
    split_ratio: float = Form(0.5),
    L: Optional[float] = Form(None),
    W: Optional[float] = Form(None),
    T: Optional[float] = Form(None),
    tol: float = Form(0.05),
):
    tmp_paths = []
    try:
        # â‘  ä¸€æ™‚ä¿å­˜
        for f in files:
            data = await f.read()
            fd, path = tempfile.mkstemp(suffix=".pdf")
            os.close(fd)
            with open(path, "wb") as w:
                w.write(data)
            tmp_paths.append((path, f.filename))

        # â‘¡ ä¸¦åˆ—å‡¦ç†ï¼ˆã“ã“ãŒè‚ï¼‰
        max_workers = min(4, os.cpu_count() or 2, len(tmp_paths))
        results = {}

        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(
                    _worker_extract_one,
                    path,
                    split_ratio,
                    L,
                    W,
                    T,
                    tol
                ): filename
                for path, filename in tmp_paths
            }

            for future in as_completed(futures):
                filename = futures[future]
                items, _debug = future.result()

                part_numbers = sorted({it["part_no"] for it in items})
                results[filename] = part_numbers

        # â‘¢ ãƒ•ãƒ­ãƒ³ãƒˆäº’æ›ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        return [
            {
                "file_name": filename,
                "count": len(results.get(filename, [])),
                "part_numbers": results.get(filename, []),
            }
            for _, filename in tmp_paths
        ]

    finally:
        for p, _ in tmp_paths:
            try:
                os.remove(p)
            except Exception:
                pass

